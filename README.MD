## Deploying Hadoop and Spark on Raspberry Pi Cluster
This a step-by-step instruction on how to deploying the distributed computing system Hadoop and Spark on a n-node Raspberry Pi cluster. It is a great practice that get you familiar with the setups and functions of the distributed computing system. If you are interested in Data Engineering with direct access to cloud computing system such AWS, this setups would a great portable alternatives.

### Hardware
To balance between cost and performance, I chose four Raspberry Pi 4B as the basic nodes. Each Pi were equipped with 1.5GHz 4-core ARM CPU, 4GB memory, and 32GB microSD card to hosting OS, related software, and storing the data. By overly charging the CPU, each CPU was clocked at 1.8GHz with PoE power.

### OS
Because this is cluster, I need a destop interface to communiciate with each Raspberry Pi. I chose to install Raspbian Lite to reduce the system loads.

### Security

### Connection

### Hadoop

### Spark

### Steps
