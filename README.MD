# Deploying Hadoop and Spark on Raspberry Pi Cluster

## Introduction
This a step-by-step instruction on how to deploying the distributed computing system Hadoop and Spark on a n-node Raspberry Pi cluster. It is a great practice that get you familiar with the setups and functions of the distributed computing system. If you are interested in Data Engineering with direct access to cloud computing system such AWS, this setups would a great portable alternatives.

## Basic Setups
To balance between cost and performance, I chose four Raspberry Pi 4B as the basic nodes. The 4-core CPU is running at 1.5Ghz by default. By tweaking the voltage to 2V, I was able to run the CPU at 1.8Ghz. To o test if the CPU is stable and cooling system is efficient during intensive workload. I run 

### Specs and cost of Respberry Pi 4 Cluster:
  * 1.8GHz 4-core CPU with 4GB memory ($57 each)
  * 32 GB Samsung EVO microSD ($7 each)
  * PoE HAT ($20 each)
  * Unifi 8 Port Switch 60W ($112)
  * Raspbian Lite OS (surprisingly free)
  * GeekPi cluster case ($39.99 not recommended)
  * 4 1-feet ethernet cables ($10)

## Security

## Connection

## Hadoop

## Spark

## Steps

## Final thoughts

## Reference
